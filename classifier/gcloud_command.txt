
### Predicting income with the Census Income Dataset using Keras

This is the Open Source Keras version of the Census sample. The sample runs both as a
standalone Keras code and on Cloud ML Engine.

## Download the data
The [Census Income DataSet](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this sample
uses for training is hosted by the [UC Irvine Machine Learning
Repository](https://archive.ics.uci.edu/ml/datasets/). We have hosted the data
on Google Cloud Storage in a slightly cleaned form:

 * Training file is `adult.data.csv`
 * Evaluation file is `adult.test.csv`

```
TRAIN_FILE=adult.data.csv
EVAL_FILE=adult.test.csv

GCS_TRAIN_FILE=gs://cloudml-public/census/data/adult.data.csv
GCS_EVAL_FILE=gs://cloudml-public/census/data/adult.test.csv

gsutil cp $GCS_TRAIN_FILE $TRAIN_FILE
gsutil cp $GCS_EVAL_FILE $EVAL_FILE
```

## Dataset generation
python3 trainer/files_to_binary.py \
    --files ../datasets/phi/train.zip ../datasets/eric/train.zip ../datasets/shengcong/train.zip ../datasets/hidayah/train.zip \
    --out ../datasets/phi_eric_shengcong_hidayah_1/train \

python3 trainer/files_to_binary.py \
    --files ../datasets/phi/test.zip ../datasets/eric/test.zip ../datasets/shengcong/test.zip ../datasets/hidayah/test.zip \
    --out ../datasets/phi_eric_shengcong_hidayah_1/test \

python3 trainer/files_to_binary.py \
    --files ../datasets/phi/train.zip ../datasets/eric/train.zip ../datasets/shengcong/train.zip ../datasets/hidayah/train.zip \
    --out ../datasets/phi_eric_shengcong_hidayah_rad_1/train \


python3 trainer/files_to_binary.py \
    --split 0.2 \
    --files ../datasets/phi/phi.zip ../datasets/datasets/eric.zip ../datasets/datasets/shengcong.zip ../datasets/datasets/hidayah.zip ../datasets/datasets/rahdiyah.zip \
    --out ../datasets/all5/all5_02 \

python3 trainer/files_to_binary.py \
    --split 0.15 \
    --augment_count 15 \
    --files ../datasets/phi/phi.zip ../datasets/datasets/eric.zip ../datasets/datasets/shengcong.zip ../datasets/datasets/hidayah.zip ../datasets/datasets/rahdiyah.zip \
    --out ../datasets/all5_augment_02/all5_augment_02 \




## Virtual environment

Virtual environments are strongly suggested, but not required. Installing this
sample's dependencies in a new virtual environment allows you to run the sample
without changing global python packages on your system.

There are two options for the virtual environments:

 * Install [Virtual](https://virtualenv.pypa.io/en/stable/) env
   * Create virtual environment `virtualenv census_keras`
   * Activate env `source census_keras/bin/activate`
 * Install [Miniconda](https://conda.io/miniconda.html)
   * Create conda environment `conda create --name census_keras python=2.7`
   * Activate env `source activate census_keras`


## Install dependencies

 * Install [gcloud](https://cloud.google.com/sdk/gcloud/)
 * Install the python dependencies. `pip install --upgrade -r requirements.txt`

## Using local python

You can run the Keras code locally

```
JOB_DIR=census_keras
TRAIN_STEPS=2000
python trainer/train.py --train_dirs datasets/p1/train/ datasets/p2/train/ datasets/phi/train \
                         --eval_dirs datasets/phi/test \
                         --job-dir models/fake_data \
                         --epoch 3 \
                         --batch 32 \
                         --eval-frequency 1 \

                         --eval_dirs datasets/p1/test/ datasets/p2/test/ \

 *** ZIP version

                         --job-dir models/phi_only \

python trainer/train.py  --train_dirs ../datasets/shengcong/train.zip ../datasets/eric/train.zip ../datasets/hidayah/train.zip ../datasets/p1/train.zip ../datasets/p2/train.zip \


python trainer/train.py  --train_dirs ../datasets/hidayah/train.zip \
                         --eval_dirs ../datasets/hidayah/test.zip \

python trainer/train.py  --train_dirs ../datasets/phi/train.zip ../datasets/shengcong/train.zip ../datasets/eric/train.zip ../datasets/hidayah/train.zip \
                         --eval_dirs ../datasets/phi/test.zip  ../datasets/shengcong/test.zip ../datasets/eric/test.zip ../datasets/hidayah/test.zip\
                         --job-dir models/fours_reduce \
                         --get_zip \
                         --epoch 10 \
                         --batch 32 \
                         --eval-frequency 1 \

                         --one_shot_freq 3 \

                         --eval_dirs datasets/p1/test/ datasets/p2/test/ \

*** NPZ version
python3 trainer/train.py --epoch 4 --batch 32 --eval-frequency 1 \
        --train_dirs ../datasets/phi_eric_shengcong_hidayah_1/train.npz \
        --eval_dirs ../datasets/phi_eric_shengcong_hidayah_1/test.npz \
        --job-dir models/fours_npz_nobn \

python3 trainer/train.py --epoch 2 --batch 32 \
        --eval-frequency 1 \
        --train_dirs ../datasets/all5/all5_02_train.npz \
        --eval_dirs ../datasets/all5/all5_02_test.npz \
        --job-dir models/all5_bn \

# dense net
python3 trainer/train.py --epoch 5 --batch 32 \
        --eval-frequency 1 \
        --config_file trainer/densenet_config.json \
        --train_dirs ../datasets/all5/all5_02_train.npz \
        --eval_dirs ../datasets/all5/all5_02_test.npz \
        --job-dir models/all5_bn_densenet \


# densenet oncloud
python3 trainer/train.py --epoch 20 --batch 64 \
		--classifier densenet \
		--checkpoint_epochs 1 \
        --eval-frequency 1 \
        --config_file trainer/densenet_config.json \
        --train_dirs ../datasets/all5_augment_015/all5_augment_015_train.npz \
        --eval_dirs ../datasets/all5_augment_015/all5_augment_015_test.npz \
        --job-dir models/all5_densenet_augment_small_2 \


# resnet oncloud
python3 trainer/train.py --epoch 30 --batch 32 \
		--classifier resnet \
        --eval-frequency 1 \
        --checkpoint_epochs 1 \
        --config_file trainer/resnet_config.json \
        --train_dirs ../datasets/all5_augment_015/all5_augment_015_train.npz \
        --eval_dirs ../datasets/all5_augment_015/all5_augment_015_test.npz \
        --job-dir models/all5_bn_densenet_augment_1 \

# normla
python3 trainer/train.py --epoch 15 --batch 64 \
		--classifier cnn_rnn \
        --eval-frequency 1 \
        --checkpoint_epochs 1 \
        --config_file trainer/resnet_config.json \
        --train_dirs ../datasets/all5_augment_015/all5_augment_015_train.npz \
        --eval_dirs ../datasets/all5_augment_015/all5_augment_015_test.npz \
        --job-dir models/all5_bn_cnn_rnn_augment_2 \

# cnn_rnn_raw_clooud
python3 trainer/train.py --epoch 20 --batch 64 \
		--classifier cnn_rnn_raw \
        --eval-frequency 1 \
        --checkpoint_epochs 1 \
        --config_file trainer/resnet_config.json \
        --train_dirs ../datasets/all5_augment_015/all5_augment_015_train.npz \
        --eval_dirs ../datasets/all5_augment_015/all5_augment_015_test.npz \
        --job-dir models/all5_cnn_rnn_raw_augment_1 \


# test on mac cnn_rnn
python3 trainer/train.py --epoch 1 --batch 32 \
		--classifier cnn_rnn_raw \
        --eval-frequency 1 \
        --checkpoint_epochs 1 \
        --config_file trainer/densenet_config.json \
        --train_dirs ../datasets/phi/test_phi.npz \
        --eval_dirs ../datasets/phi/test_phi.npz \
        --job-dir models/test_cnn_rnn \

python3 trainer/train.py --epoch 5 --batch 32 \
		--classifier cnn_rnn_raw \
        --eval-frequency 1 \
        --checkpoint_epochs 1 \
        --config_file trainer/densenet_config.json \
        --train_dirs ../datasets/phi/test_phi.npz \
        --eval_dirs ../datasets/phi/test_phi.npz \
        --job-dir models/test_cnn_rnn_raw \

python3 trainer/train.py --epoch 15 --batch 32 \
		--classifier cnn_rnn_raw \
        --eval-frequency 1 \
        --checkpoint_epochs 1 \
        --config_file trainer/densenet_config.json \
        --train_dirs ../datasets/phi/test_phi.npz \
        --eval_dirs ../datasets/phi/test_phi.npz \
        --job-dir models/test_cnn_rnn_raw \



```

## Training using gcloud local

You can run Keras training using gcloud locally

```
JOB_DIR=census_keras
TRAIN_STEPS=200
gcloud ml-engine local train --package-path trainer \
                             --module-name trainer.train \
                             -- \
                             --train_dirs datasets/p1/train/ datasets/p2/train/ \
                             --eval_dirs datasets/phi/test \
                             --job-dir models/fake_data \
                             --epoch 3 \
                             --batch 64 \
                             --eval-frequency 1 \

                             --eval_dirs datasets/p1/test/ datasets/p2/test/ \

```

## Prediction using gcloud local

You can run prediction on the SavedModel created from Keras HDF5 model

```
python preprocess.py sample.json
```

```
gcloud ml-engine local predict --model-dir=$JOB_DIR/export \
                               --json-instances sample.json
```

## Training using Cloud ML Engine

You can train the model on Cloud ML Engine

```
SCALE_TIER=BASIC_GPU
JOB_NAME=eye_fake_oneshot_p1_p2_phi__p1_p2_phi_9 && SCALE_TIER=STANDARD_1 && gcloud ml-engine jobs submit training $JOB_NAME \
                                    --stream-logs \
                                    --scale-tier=$SCALE_TIER \
                                    --runtime-version 1.2 \
                                    --job-dir gs://phi/eye_job_dir/$JOB_NAME \
                                    --package-path trainer \
                                    --module-name trainer.train \
                                    --region asia-east1 \
                                    -- \
                                    --train_dirs gs://phi/eye_data/p1/train.zip gs://phi/eye_data/p2/train.zip gs://phi/eye_data/phi/train.zip \
                                    --eval_dirs gs://phi/eye_data/p1/test.zip gs://phi/eye_data/p2/test.zip gs://phi/eye_data/phi/test.zip \
                                    --get_zip \
                                    --epoch 15 \
                                    --batch 64 \
                                    --eval-frequency 2 \
                                    --check-point_epochs 2 \
                                    --one_shot_freq 50 \

```

## Prediction using Cloud ML Engine

You can perform prediction on Cloud ML Engine by following the steps below.
Create a model on Cloud ML Engine

```
gcloud ml-engine models create keras_model --regions us-central1
```

Export the model binaries

```
MODEL_BINARIES=$JOB_DIR/export
```

Deploy the model to the prediction service

```
gcloud ml-engine versions create v1 --model keras_model --origin $MODEL_BINARIES --runtime-version 1.2
```

Create a processed sample from the data

```
python preprocess.py sample.json

```

Run the online prediction

```
gcloud ml-engine predict --model keras_model --version v1 --json-instances sample.json
```